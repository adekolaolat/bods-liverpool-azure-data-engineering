{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6f255b40-6063-4286-a920-7c7bad66f4e7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "silver_path = \"abfss://silver@livbusdatastore.dfs.core.windows.net/\"\n",
    "\n",
    "ServicePrincipalId = dbutils.secrets.get(scope=\"livbodsbus-keyvault\",key=\"dbx-client-ID\")\n",
    "ServicePrincipalKey = dbutils.secrets.get(scope=\"livbodsbus-keyvault\",key=\"dbx-secret\")\n",
    "TenantId = dbutils.secrets.get(scope=\"livbodsbus-keyvault\",key=\"dbx-tenant-ID\")\n",
    "\n",
    "\n",
    "# Configure access to ADLS Gen2\n",
    "spark.conf.set(\"fs.azure.account.auth.type.livbusdatastore.dfs.core.windows.net\", \"OAuth\")\n",
    "spark.conf.set(\"fs.azure.account.oauth.provider.type.livbusdatastore.dfs.core.windows.net\", \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\")\n",
    "spark.conf.set(\"fs.azure.account.oauth2.client.id.livbusdatastore.dfs.core.windows.net\", ServicePrincipalId)\n",
    "spark.conf.set(\"fs.azure.account.oauth2.client.secret.livbusdatastore.dfs.core.windows.net\", ServicePrincipalKey)\n",
    "spark.conf.set(\"fs.azure.account.oauth2.client.endpoint.livbusdatastore.dfs.core.windows.net\", f\"https://login.microsoftonline.com/{TenantId}/oauth2/token\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "03c3e247-1d40-4d6a-a46d-47f16dfa192e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Update previous day gold bus_activity to be used for analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "69d6c9b6-c217-4afd-b25e-c7c85918f227",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, lag, when, unix_timestamp, to_date, sum, year, month, dayofmonth, date_sub, current_date, row_number\n",
    "from pyspark.sql.window import Window\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "# 1. Set dates\n",
    "yesterday_date = date_sub(current_date(), 1)\n",
    "day_before_yesterday = date_sub(current_date(), 2)\n",
    "\n",
    "yesterday_year = year(yesterday_date)\n",
    "yesterday_month = month(yesterday_date)\n",
    "yesterday_day = dayofmonth(yesterday_date)\n",
    "\n",
    "# 2. Load yesterday's data using partition pruning\n",
    "df_yesterday = spark.read.format(\"delta\").table(\"silver.bus_activity\") \\\n",
    "    .filter(\n",
    "        (col(\"year\") == yesterday_year) &\n",
    "        (col(\"month\") == yesterday_month) &\n",
    "        (col(\"day\") == yesterday_day)\n",
    "    )\n",
    "\n",
    "# 3. Load last record per vehicle from day before yesterday\n",
    "df_prev = spark.read.format(\"delta\").table(\"silver.bus_activity\") \\\n",
    "    .filter(\n",
    "        (col(\"year\") == year(day_before_yesterday)) &\n",
    "        (col(\"month\") == month(day_before_yesterday)) &\n",
    "        (col(\"day\") == dayofmonth(day_before_yesterday))\n",
    "    )\n",
    "\n",
    "window_desc = Window.partitionBy(\"vehicle_ref\").orderBy(col(\"ingestion_timestamp\").desc())\n",
    "df_prev_latest = df_prev.withColumn(\"rn\", row_number().over(window_desc)) \\\n",
    "                        .filter(col(\"rn\") == 1).drop(\"rn\")\n",
    "\n",
    "# 4. Combine both datasets\n",
    "df = df_yesterday.unionByName(df_prev_latest)\n",
    "\n",
    "# 5. Define Liverpool bounding box\n",
    "liverpool_min_lat, liverpool_max_lat = 53.33, 53.50\n",
    "liverpool_min_lon, liverpool_max_lon = -3.00, -2.86\n",
    "\n",
    "# 6. Add 'in_liverpool' column\n",
    "df = df.withColumn(\n",
    "    \"in_liverpool\",\n",
    "    when(\n",
    "        (col(\"latitude\").between(liverpool_min_lat, liverpool_max_lat)) &\n",
    "        (col(\"longitude\").between(liverpool_min_lon, liverpool_max_lon)),\n",
    "        True\n",
    "    ).otherwise(False)\n",
    ")\n",
    "\n",
    "# 7. Define window by vehicle and time\n",
    "vehicle_window = Window.partitionBy(\"vehicle_ref\").orderBy(\"ingestion_timestamp\")\n",
    "\n",
    "# 8. Lag previous values\n",
    "df = df.withColumn(\"prev_latitude\", lag(\"latitude\").over(vehicle_window)) \\\n",
    "       .withColumn(\"prev_longitude\", lag(\"longitude\").over(vehicle_window)) \\\n",
    "       .withColumn(\"prev_in_liverpool\", lag(\"in_liverpool\").over(vehicle_window)) \\\n",
    "       .withColumn(\"prev_recorded_time\", lag(\"recorded_at_time\").over(vehicle_window))\n",
    "\n",
    "# 9. Calculate duration in minutes\n",
    "df = df.withColumn(\n",
    "    \"dur_min_since_last_recorded\",\n",
    "    (unix_timestamp(\"recorded_at_time\") - unix_timestamp(\"prev_recorded_time\")) / 60\n",
    ")\n",
    "\n",
    "# 10. Detect idle state\n",
    "df = df.withColumn(\n",
    "    \"possibly_idle\",\n",
    "    when(\n",
    "        ((col(\"latitude\") == col(\"prev_latitude\")) & \n",
    "         (col(\"longitude\") == col(\"prev_longitude\"))) |\n",
    "         (col(\"recorded_at_time\") == col(\"prev_recorded_time\")),\n",
    "        True\n",
    "    ).otherwise(False)\n",
    ")\n",
    "\n",
    "# 11. Label Liverpool movement\n",
    "df = df.withColumn(\n",
    "    \"liverpool_movement_status\",\n",
    "    when((col(\"prev_in_liverpool\").isNull()) & (col(\"in_liverpool\") == True), \"Only_In_Liverpool\")\n",
    "    .when(col(\"prev_in_liverpool\") != col(\"in_liverpool\"), \"Moved_In_Or_Out\")\n",
    "    .otherwise(\"Other\")\n",
    ")\n",
    "\n",
    "# 12. Infer routes that visit Liverpool\n",
    "liverpool_routes = df.filter(col(\"in_liverpool\") == True) \\\n",
    "                     .select(\"line_ref\", \"operator_ref\") \\\n",
    "                     .distinct()\n",
    "\n",
    "# 13. Join back to get data for Liverpool-related routes\n",
    "df_liverpool_routes = df.join(liverpool_routes, on=[\"line_ref\", \"operator_ref\"], how=\"inner\")\n",
    "\n",
    "# 14. Apply filter rules\n",
    "bus_rules_path = \"abfss://config@livbusdatastore.dfs.core.windows.net/bus_line_rules.json\"\n",
    "line_rules = spark.read.option(\"multiline\", \"true\").json(bus_rules_path).collect()[0]\n",
    "\n",
    "lines_to_remove = line_rules[\"lines_to_exclude_completely\"]\n",
    "\n",
    "df_final = df_liverpool_routes.filter(~col(\"line_ref\").isin(lines_to_remove))\n",
    "\n",
    "for rule in line_rules[\"special_filters\"]:\n",
    "    lines = rule[\"lines\"]\n",
    "    lat_condition = rule[\"latitude_condition\"]\n",
    "    lat_value = rule[\"latitude_value\"]\n",
    "    \n",
    "    if lat_condition == \">\":\n",
    "        df_final = df_final.filter(\n",
    "            ~(\n",
    "                (col(\"line_ref\").isin(lines)) & (col(\"latitude\") > lat_value)\n",
    "            )\n",
    "        )\n",
    "    elif lat_condition == \"<\":\n",
    "        df_final = df_final.filter(\n",
    "            ~(\n",
    "                (col(\"line_ref\").isin(lines)) & (col(\"latitude\") < lat_value)\n",
    "            )\n",
    "        )\n",
    "\n",
    "# 15. Add ingestion_date\n",
    "df_bus_activity = df_final.withColumn(\"ingestion_date\", to_date(col(\"ingestion_timestamp\")))\n",
    "\n",
    "# 16. Remove duplicates for yesterday\n",
    "bus_activity_path = \"abfss://gold@livbusdatastore.dfs.core.windows.net/bus_activity\"\n",
    "\n",
    "if DeltaTable.isDeltaTable(spark, bus_activity_path):\n",
    "    gold_table = DeltaTable.forPath(spark, bus_activity_path)\n",
    "    gold_table.delete(f\"ingestion_date = date_sub(current_date(), 1)\")\n",
    "\n",
    "# 17. Write enriched data to Gold\n",
    "df_bus_activity.write.format(\"delta\") \\\n",
    "    .mode(\"append\") \\\n",
    "    .partitionBy(\"ingestion_date\") \\\n",
    "    .save(bus_activity_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "487d0723-6883-40f0-b011-1e33066e5fc2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Update bus trip counts in gold table**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e210a263-9611-4622-acc6-876025f5d532",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, lag, when, unix_timestamp, sum, to_date, date_sub, current_date\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Step 1: Get yesterday's date\n",
    "yesterday_date = date_sub(current_date(), 1)\n",
    "\n",
    "# Step 2: Read ONLY yesterday's data from gold.bus_activity\n",
    "df = (\n",
    "    spark.read.format(\"delta\")\n",
    "    .table(\"gold.bus_activity\")\n",
    "    .filter(col(\"ingestion_date\") == yesterday_date)\n",
    ")\n",
    "\n",
    "# Step 3: Define the window\n",
    "trip_window = Window.partitionBy(\"vehicle_ref\", \"line_ref\", \"ingestion_date\").orderBy(\"ingestion_timestamp\")\n",
    "\n",
    "# Step 4: Lag previous direction\n",
    "df = df.withColumn(\"prev_direction_ref\", lag(\"direction_ref\").over(trip_window))\n",
    "\n",
    "# Step 5: Detect direction changes\n",
    "df = df.withColumn(\n",
    "    \"trip_start\",\n",
    "    (col(\"direction_ref\") != col(\"prev_direction_ref\")).cast(\"int\")  # 1 if direction changed, else 0\n",
    ")\n",
    "\n",
    "# Step 6: Sum the trip_start flags per bus per line per day\n",
    "df_trip_counts = df.groupBy(\"vehicle_ref\", \"line_ref\", \"ingestion_date\", \"operator_ref\") \\\n",
    "    .agg(\n",
    "        sum(\"trip_start\").alias(\"trip_count\")\n",
    "    )\n",
    "\n",
    "# Step 7: Load the gold trip_counts table (assuming you already created it)\n",
    "bus_trips_path = \"abfss://gold@livbusdatastore.dfs.core.windows.net/bus_trips_counts\"\n",
    "\n",
    "# If not already loaded\n",
    "trip_counts_table = DeltaTable.forPath(spark, bus_trips_path)\n",
    "\n",
    "# Step 8: Delete old data for yesterday (if it exists)\n",
    "trip_counts_table.delete(\"ingestion_date = date_sub(current_date(), 1)\")\n",
    "\n",
    "# Step 9: Append the new trip_counts\n",
    "df_trip_counts.write.format(\"delta\") \\\n",
    "    .mode(\"append\") \\\n",
    "    .option(\"mergeSchema\", \"true\") \\\n",
    "    .save(bus_trips_path)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 4519191953678241,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Gold Notebook",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
