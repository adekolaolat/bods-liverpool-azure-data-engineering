{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "921b730d-ac87-443d-ae54-dafbfbebdd40",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Configure connection to ADLS Gen2 using Service Principal**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ca7c93a9-f38c-499c-b618-3aa78e8a84de",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "bronze_path = \"abfss://bronze@livbusdatastore.dfs.core.windows.net/\"\n",
    "silver_path = \"abfss://silver@livbusdatastore.dfs.core.windows.net/\"\n",
    "\n",
    "ServicePrincipalId = dbutils.secrets.get(scope=\"livbodsbus-keyvault\",key=\"dbx-client-ID\")\n",
    "ServicePrincipalKey = dbutils.secrets.get(scope=\"livbodsbus-keyvault\",key=\"dbx-secret\")\n",
    "TenantId = dbutils.secrets.get(scope=\"livbodsbus-keyvault\",key=\"dbx-tenant-ID\")\n",
    "\n",
    "\n",
    "# Configure access to ADLS Gen2\n",
    "spark.conf.set(\"fs.azure.account.auth.type.livbusdatastore.dfs.core.windows.net\", \"OAuth\")\n",
    "spark.conf.set(\"fs.azure.account.oauth.provider.type.livbusdatastore.dfs.core.windows.net\", \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\")\n",
    "spark.conf.set(\"fs.azure.account.oauth2.client.id.livbusdatastore.dfs.core.windows.net\", ServicePrincipalId)\n",
    "spark.conf.set(\"fs.azure.account.oauth2.client.secret.livbusdatastore.dfs.core.windows.net\", ServicePrincipalKey)\n",
    "spark.conf.set(\"fs.azure.account.oauth2.client.endpoint.livbusdatastore.dfs.core.windows.net\", f\"https://login.microsoftonline.com/{TenantId}/oauth2/token\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "33577bfd-de27-40dc-91b0-408ecd59881b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Batch loading of files with Autoloader**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "40fe7fa7-ee4b-479a-9f97-0fce0b070399",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Define the bronze directory path for the operator\n",
    "bronze_path = \"abfss://bronze@livbusdatastore.dfs.core.windows.net/operator=*/year=*/month=*/day=*\"\n",
    "\n",
    "# Define the schema location path (this is where AutoLoader will store schema metadata)\n",
    "schema_location = \"abfss://bronze@livbusdatastore.dfs.core.windows.net/schemas/bus_activity_schema/\"\n",
    "\n",
    "# STEP 1: Read data from the bronze directory using Auto Loader for batch processing\n",
    "\n",
    "MaxFilesPerTrigger = 600\n",
    "\n",
    "# AutoLoader will automatically detect new files as they arrive in the source folder\n",
    "df_batch = spark.readStream.format(\"cloudFiles\") \\\n",
    "    .option(\"cloudFiles.format\", \"json\") \\\n",
    "    .option(\"cloudFiles.inferColumnTypes\", \"true\") \\\n",
    "    .option(\"cloudFiles.schemaLocation\", schema_location) \\\n",
    "    .option(\"cloudFiles.maxFilesPerTrigger\", MaxFilesPerTrigger)\\\n",
    "    .load(bronze_path)\n",
    "\n",
    "# Step 2: Add file timestamp and file path_name\n",
    "df_with_metadata = df_batch.withColumn(\"file_path\", input_file_name()) \\\n",
    "    .withColumn(\"file_timestamp\", regexp_extract(\"file_path\", r'_(\\d{8}_\\d{6})\\.json', 1))\n",
    "\n",
    "# Step 3: Extract operator from file path (useful for routing logic)\n",
    "df_with_operator = df_with_metadata.withColumn(\"operator_extracted\", regexp_extract(\"file_path\", r'operator=([^/]+)', 1))\n",
    "\n",
    "# Step 4: Try parsing the `Siri` column (which is already a struct if schema inference worked correctly)\n",
    "# Just alias it cleanly for further processing\n",
    "df_siri = df_with_operator.select(\n",
    "    col(\"Siri.*\"),  # Unpack top-level fields inside 'Siri'\n",
    "    col(\"operator_extracted\").alias(\"operator\"),\n",
    "    \"file_timestamp\",\n",
    "    \"file_path\"\n",
    ")\n",
    "df_flat = df_siri.select(\n",
    "    explode(col(\"ServiceDelivery.VehicleMonitoringDelivery.VehicleActivity\")).alias(\"activity\"),\n",
    "    \"file_timestamp\"\n",
    ")\n",
    "# df_flat.display()\n",
    "\n",
    "# STEP 5: Flatten the nested JSON structure\n",
    "df_selected = df_flat.select(\n",
    "    col(\"activity.RecordedAtTime\").alias(\"recorded_at_time\"),\n",
    "    col(\"activity.ItemIdentifier\").alias(\"item_identifier\"),\n",
    "    col(\"activity.ValidUntilTime\").alias(\"valid_until_time\"),\n",
    "\n",
    "    col(\"activity.MonitoredVehicleJourney.LineRef\").alias(\"line_ref\"),\n",
    "    col(\"activity.MonitoredVehicleJourney.DirectionRef\").alias(\"direction_ref\"),\n",
    "    col(\"activity.MonitoredVehicleJourney.FramedVehicleJourneyRef.DataFrameRef\").alias(\"data_frame_ref\"),\n",
    "    col(\"activity.MonitoredVehicleJourney.FramedVehicleJourneyRef.DatedVehicleJourneyRef\").cast(\"string\").alias(\"dated_vehicle_journey_ref\"),\n",
    "    col(\"activity.MonitoredVehicleJourney.PublishedLineName\").alias(\"published_line_name\"),\n",
    "    col(\"activity.MonitoredVehicleJourney.OperatorRef\").alias(\"operator_ref\"),\n",
    "    col(\"activity.MonitoredVehicleJourney.OriginRef\").cast(\"string\").alias(\"origin_ref\"),\n",
    "    col(\"activity.MonitoredVehicleJourney.OriginName\").alias(\"origin_name\"),\n",
    "    col(\"activity.MonitoredVehicleJourney.DestinationRef\").cast(\"string\").alias(\"destination_ref\"),\n",
    "    col(\"activity.MonitoredVehicleJourney.DestinationName\").alias(\"destination_name\"),\n",
    "    col(\"activity.MonitoredVehicleJourney.OriginAimedDepartureTime\").alias(\"origin_aimed_departure_time\"),\n",
    "    col(\"activity.MonitoredVehicleJourney.DestinationAimedArrivalTime\").alias(\"destination_aimed_arrival_time\"),\n",
    "\n",
    "    col(\"activity.MonitoredVehicleJourney.VehicleLocation.Longitude\").alias(\"longitude\"),\n",
    "    col(\"activity.MonitoredVehicleJourney.VehicleLocation.Latitude\").alias(\"latitude\"),\n",
    "    col(\"activity.MonitoredVehicleJourney.BlockRef\").alias(\"block_ref\"),\n",
    "    col(\"activity.MonitoredVehicleJourney.VehicleRef\").cast(\"string\").alias(\"vehicle_ref\"),\n",
    "\n",
    "    # Handle optional fields with `when` for safe access\n",
    "    when(col(\"activity.Extensions.VehicleJourney.Operational.TicketMachine.TicketMachineServiceCode\").isNotNull(),\n",
    "         col(\"activity.Extensions.VehicleJourney.Operational.TicketMachine.TicketMachineServiceCode\"))\n",
    "    .otherwise(lit(None)).alias(\"ticket_machine_service_code\"),\n",
    "\n",
    "    when(col(\"activity.Extensions.VehicleJourney.Operational.TicketMachine.JourneyCode\").isNotNull(),\n",
    "         col(\"activity.Extensions.VehicleJourney.Operational.TicketMachine.JourneyCode\"))\n",
    "    .otherwise(lit(None)).alias(\"journey_code\"),\n",
    "\n",
    "    when(col(\"activity.Extensions.VehicleJourney.VehicleUniqueId\").isNotNull(),\n",
    "         col(\"activity.Extensions.VehicleJourney.VehicleUniqueId\").cast(\"string\"))\n",
    "    .otherwise(lit(None)).alias(\"vehicle_unique_id\"),\n",
    "\n",
    "    col(\"activity.MonitoredVehicleJourney.Bearing\").alias(\"bearing\"),\n",
    "    \n",
    "    # Handle nested fields like `Monitored`\n",
    "    when(col(\"activity.MonitoredVehicleJourney.Monitored\").isNotNull(),\n",
    "         col(\"activity.MonitoredVehicleJourney.Monitored\"))\n",
    "    .otherwise(lit(None).cast(\"boolean\")).alias(\"monitored\"),\n",
    "\n",
    "    when(col(\"activity.Extensions.VehicleJourney.DriverRef\").isNotNull(),\n",
    "         col(\"activity.Extensions.VehicleJourney.DriverRef\").cast(\"string\"))\n",
    "    .otherwise(lit(None)).alias(\"driver_ref\"),\n",
    "\n",
    "    col(\"file_timestamp\"),\n",
    ").withColumn(\"ingestion_timestamp\", to_timestamp(col(\"file_timestamp\"), \"yyyyMMdd_HHmmss\").cast(\"string\"))\\\n",
    ".withColumn(\"year\", year(\"ingestion_timestamp\")) \\\n",
    ".withColumn(\"month\", month(\"ingestion_timestamp\")) \\\n",
    ".withColumn(\"day\", dayofmonth(\"ingestion_timestamp\"))\n",
    "\n",
    "checkpoint_path = \"abfss://silver@livbusdatastore.dfs.core.windows.net/_checkpoints/bus_activity/\"\n",
    "output_path = \"abfss://silver@livbusdatastore.dfs.core.windows.net/bus_activity/\"\n",
    "\n",
    "# # STEP 6: Write the result to Delta Lake (Silver layer)\n",
    "query = (\n",
    "    df_selected.writeStream\n",
    "    .format(\"delta\")\n",
    "    .outputMode(\"append\")\n",
    "    .option(\"checkpointLocation\", checkpoint_path)\n",
    "    .partitionBy(\"operator_ref\", \"year\", \"month\", \"day\")\n",
    "    .trigger(once=True)\n",
    "    .start(output_path)\n",
    ")\n",
    "\n",
    "query.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6f24cb5d-45bf-4eb7-8eee-b0e5ff2eb1ea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Register as table for SQL access\n",
    "spark.sql(\"CREATE SCHEMA IF NOT EXISTS silver\")\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "  CREATE TABLE IF NOT EXISTS silver.bus_activity\n",
    "  USING DELTA\n",
    "  LOCATION '{output_path}'\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8789334469123250,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Silver Notebook",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
