{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6f255b40-6063-4286-a920-7c7bad66f4e7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "silver_path = \"abfss://silver@livbusdatastore.dfs.core.windows.net/\"\n",
    "\n",
    "ServicePrincipalId = dbutils.secrets.get(scope=\"livbodsbus-keyvault\",key=\"dbx-client-ID\")\n",
    "ServicePrincipalKey = dbutils.secrets.get(scope=\"livbodsbus-keyvault\",key=\"dbx-secret\")\n",
    "TenantId = dbutils.secrets.get(scope=\"livbodsbus-keyvault\",key=\"dbx-tenant-ID\")\n",
    "\n",
    "\n",
    "# Configure access to ADLS Gen2\n",
    "spark.conf.set(\"fs.azure.account.auth.type.livbusdatastore.dfs.core.windows.net\", \"OAuth\")\n",
    "spark.conf.set(\"fs.azure.account.oauth.provider.type.livbusdatastore.dfs.core.windows.net\", \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\")\n",
    "spark.conf.set(\"fs.azure.account.oauth2.client.id.livbusdatastore.dfs.core.windows.net\", ServicePrincipalId)\n",
    "spark.conf.set(\"fs.azure.account.oauth2.client.secret.livbusdatastore.dfs.core.windows.net\", ServicePrincipalKey)\n",
    "spark.conf.set(\"fs.azure.account.oauth2.client.endpoint.livbusdatastore.dfs.core.windows.net\", f\"https://login.microsoftonline.com/{TenantId}/oauth2/token\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "03c3e247-1d40-4d6a-a46d-47f16dfa192e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "**Perform initial batch loading of historical data.**\n",
    "\n",
    "**Enrich the `silver.bus_activity` table by adding additional identifier columns to prepare for building the `gold.bus_activity` table, which will be used for analysis.**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "69d6c9b6-c217-4afd-b25e-c7c85918f227",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, lag, when, unix_timestamp, to_date,sum\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Load the base data\n",
    "df = spark.read.format(\"delta\").table(\"silver.bus_activity\")\n",
    "\n",
    "# 1. Define bounding box for Liverpool\n",
    "liverpool_min_lat, liverpool_max_lat = 53.33, 53.50\n",
    "liverpool_min_lon, liverpool_max_lon = -3.00, -2.86\n",
    "\n",
    "# 2. Add 'in_liverpool' column\n",
    "df = df.withColumn(\n",
    "    \"in_liverpool\",\n",
    "    when(\n",
    "        (col(\"latitude\").between(liverpool_min_lat, liverpool_max_lat)) &\n",
    "        (col(\"longitude\").between(liverpool_min_lon, liverpool_max_lon)),\n",
    "        True\n",
    "    ).otherwise(False)\n",
    ")\n",
    "\n",
    "# 3. Define window by vehicle and time\n",
    "vehicle_window = Window.partitionBy(\"vehicle_ref\").orderBy(\"ingestion_timestamp\")\n",
    "\n",
    "# 4. Lag previous values\n",
    "df = df.withColumn(\"prev_latitude\", lag(\"latitude\").over(vehicle_window)) \\\n",
    "       .withColumn(\"prev_longitude\", lag(\"longitude\").over(vehicle_window)) \\\n",
    "       .withColumn(\"prev_in_liverpool\", lag(\"in_liverpool\").over(vehicle_window)) \\\n",
    "       .withColumn(\"prev_recorded_time\", lag(\"recorded_at_time\").over(vehicle_window))\n",
    "\n",
    "# 5. Calculate duration in minutes\n",
    "df = df.withColumn(\n",
    "    \"dur_min_since_last_recorded\",\n",
    "    (unix_timestamp(\"recorded_at_time\") - unix_timestamp(\"prev_recorded_time\")) / 60\n",
    ")\n",
    "\n",
    "# 6. Detect idle state\n",
    "df = df.withColumn(\n",
    "    \"possibly_idle\",\n",
    "    when(\n",
    "        ((col(\"latitude\") == col(\"prev_latitude\")) & \n",
    "         (col(\"longitude\") == col(\"prev_longitude\"))) |\n",
    "         (col(\"recorded_at_time\") == col(\"prev_recorded_time\")),\n",
    "        True\n",
    "    ).otherwise(False)\n",
    ")\n",
    "\n",
    "# 7. Label Liverpool movement\n",
    "df = df.withColumn(\n",
    "    \"liverpool_movement_status\",\n",
    "    when((col(\"prev_in_liverpool\").isNull()) & (col(\"in_liverpool\") == True), \"Only_In_Liverpool\")\n",
    "    .when(col(\"prev_in_liverpool\") != col(\"in_liverpool\"), \"Moved_In_Or_Out\")\n",
    "    .otherwise(\"Other\")\n",
    ")\n",
    "\n",
    "# 8. Infer routes that visit Liverpool\n",
    "liverpool_routes = df.filter(col(\"in_liverpool\") == True) \\\n",
    "                     .select(\"line_ref\", \"operator_ref\") \\\n",
    "                     .distinct()\n",
    "\n",
    "# 9. Join back to get data for vehicles running on Liverpool-related routes\n",
    "df_liverpool_routes = df.join(liverpool_routes, on=[\"line_ref\", \"operator_ref\"], how=\"inner\")\n",
    "\n",
    "\n",
    "# Filter bus lines inside/outside Liverpool with the rules in config container \n",
    "bus_rules_path = \"abfss://config@livbusdatastore.dfs.core.windows.net/bus_line_rules.json\"\n",
    "line_rules = spark.read.option(\"multiline\", \"true\").json(bus_rules_path).collect()[0]\n",
    "\n",
    "lines_to_remove = line_rules [\"lines_to_exclude_completely\"]\n",
    "\n",
    "df_final = df_liverpool_routes.filter(~col(\"line_ref\").isin(lines_to_remove))\n",
    "\n",
    "\n",
    "# Apply special latitude filters for some lines that pop up in Wirral and Prescot\n",
    "for rule in line_rules[\"special_filters\"]:\n",
    "    lines = rule[\"lines\"]\n",
    "    lat_condition = rule[\"latitude_condition\"]\n",
    "    lat_value = rule[\"latitude_value\"]\n",
    "    \n",
    "    if lat_condition == \">\":\n",
    "        df_final = df_final.filter(\n",
    "            ~(\n",
    "                (col(\"line_ref\").isin(lines)) & (col(\"latitude\") > lat_value)\n",
    "            )\n",
    "        )\n",
    "    elif lat_condition == \"<\":\n",
    "        df_final = df_final.filter(\n",
    "            ~(\n",
    "                (col(\"line_ref\").isin(lines)) & (col(\"latitude\") < lat_value)\n",
    "            )\n",
    "        )\n",
    "\n",
    "# Add date column\n",
    "df_bus_activity =df_final.withColumn(\"ingestion_date\", to_date(col(\"ingestion_timestamp\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "85c89ff4-60a3-40a9-a17b-0fc84649fff0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "bus_activity_path = \"abfss://gold@livbusdatastore.dfs.core.windows.net/bus_activity\"\n",
    "\n",
    "df_bus_activity.write.format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .partitionBy(\"ingestion_date\") \\\n",
    "    .save(bus_activity_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dd4b15be-c2db-48e9-8283-79ceb5491b11",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Register access to the bus_activity table\n",
    "spark.sql(\"CREATE SCHEMA IF NOT EXISTS gold\")\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "  CREATE TABLE IF NOT EXISTS gold.bus_activity\n",
    "  USING DELTA\n",
    "  LOCATION \"abfss://gold@livbusdatastore.dfs.core.windows.net/bus_activity\"\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b990ae02-c27c-408e-8258-45d77c625baf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "**Perform initial batch loading for counting the daily trips by each vehicle.**\n",
    "\n",
    "**Create delta table to monitor trip activity from `gold.busactivity`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2c674868-7d7c-4456-974e-6a20a556b2f5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, lag, when, unix_timestamp, sum\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Step 1: Read the base table\n",
    "df = spark.read.format(\"delta\").table(\"gold.bus_activity\")\n",
    "\n",
    "# Step 2: Define a window\n",
    "trip_window = Window.partitionBy(\"vehicle_ref\", \"line_ref\", \"ingestion_date\").orderBy(\"ingestion_timestamp\")\n",
    "\n",
    "# Step 3: Lag previous direction\n",
    "df = df.withColumn(\"prev_direction_ref\", lag(\"direction_ref\").over(trip_window))\n",
    "\n",
    "# Step 4: Detect direction changes\n",
    "df = df.withColumn(\n",
    "    \"trip_start\",\n",
    "    (col(\"direction_ref\") != col(\"prev_direction_ref\")).cast(\"int\")  # 1 if direction changed, else 0\n",
    ")\n",
    "\n",
    "# Step 5: Sum the trip_start flags per bus per line per day\n",
    "df_trip_counts = df.groupBy(\"vehicle_ref\", \"line_ref\", \"ingestion_date\", \"operator_ref\") \\\n",
    "    .agg(\n",
    "        sum(\"trip_start\").alias(\"trip_count\")\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7d4c6034-c22a-4876-980a-9ceb0a78bd90",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "bus_trips_path = \"abfss://gold@livbusdatastore.dfs.core.windows.net/bus_trips_counts\"\n",
    "\n",
    "df_trip_counts.write.format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .partitionBy(\"ingestion_date\") \\\n",
    "    .save(bus_trips_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a7e8240c-b19e-48b6-b672-206cc9dc959d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Register access to the bus_activity table\n",
    "spark.sql(\"CREATE SCHEMA IF NOT EXISTS gold\")\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "  CREATE TABLE IF NOT EXISTS gold.bus_trip_counts\n",
    "  USING DELTA\n",
    "  LOCATION \"abfss://gold@livbusdatastore.dfs.core.windows.net/bus_trips_counts\"\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 4014556607457093,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Gold Notebook - Batch Load",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
